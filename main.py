# ======================================================
# ëŒ€í•™ Â· í•™ê³¼ ì¶”ì²œ API ì„œë²„
# FastAPI + Embedding + GPT-4o-mini
# ======================================================

import pandas as pd
import numpy as np
import ast
import re
import os

from sentence_transformers import SentenceTransformer
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI
from dotenv import load_dotenv


# ======================================================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)
# ======================================================
load_dotenv()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)


# ======================================================
# 2. FastAPI ì•± ìƒì„±
# ======================================================
app = FastAPI(
    title="ëŒ€í•™ Â· í•™ê³¼ ì¶”ì²œ ì±—ë´‡ API",
    description="Embedding + GPT ê¸°ë°˜ ëŒ€í•™ ì „ê³µ ì¶”ì²œ",
    version="1.0.0"
)


# ======================================================
# 3. í…ìŠ¤íŠ¸ ì •ê·œí™”
# ======================================================
def normalize_text(text: str) -> str:
    text = str(text)
    text = re.sub(r"[,\u00b7ãƒ»]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


# ======================================================
# 4. ê°„ë‹¨í•œ ì˜ë„ ì¶”ì¶œ
# ======================================================
def extract_intent(text: str):
    regions = ["ì„œìš¸", "ê²½ê¸°", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°"]
    majors = ["ì»´í“¨í„°", "ì†Œí”„íŠ¸ì›¨ì–´", "AI", "ì¸ê³µì§€ëŠ¥", "ì •ë³´", "ë°ì´í„°"]

    region = next((r for r in regions if r in text), None)
    major = next((m for m in majors if m.lower() in text.lower()), None)

    return region, major


# ======================================================
# 5. í•™ê³¼ DB + ì„ë² ë”© ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ)
# ======================================================
print("ğŸ“‚ í•™ê³¼ ë²¡í„° DB ë¡œë”© ì¤‘...")

df = pd.read_csv("test_language.csv").fillna("")
df.columns = df.columns.str.strip()

# ğŸ”¥ ì»¬ëŸ¼ëª… ì˜¤íƒ€ ë³´ì •
df.rename(columns={"í‘œì¤€ë¶„ ë¥˜ê³„ì—´(ì†Œ)": "í‘œì¤€ë¶„ë¥˜ê³„ì—´(ì†Œ)"}, inplace=True)

# ğŸ”¥ ì§€ì—­ ì»¬ëŸ¼ í†µí•© (í•µì‹¬)
if "ì†Œì¬ì§€" in df.columns:
    df["ì§€ì—­"] = df["ì†Œì¬ì§€"]
elif "ì†Œì¬ì§€(ìƒì„¸)" in df.columns:
    df["ì§€ì—­"] = df["ì†Œì¬ì§€(ìƒì„¸)"]
else:
    df["ì§€ì—­"] = ""

# ğŸ”¥ ì„ë² ë”© ë¡œë“œ
df["embedding"] = df["embedding"].apply(
    lambda x: np.array(ast.literal_eval(x))
)
corpus_embeddings = np.vstack(df["embedding"].values)

# ğŸ”¥ ì„ë² ë”© ëª¨ë¸
model = SentenceTransformer("intfloat/multilingual-e5-base")

print("âœ… í•™ê³¼ DB ë¡œë”© ì™„ë£Œ")


# ======================================================
# 6. í•™ê³¼ ê²€ìƒ‰ ë¡œì§
# ======================================================
def search_major(user_query: str, top_k: int = 3):
    query = "query: " + normalize_text(user_query)

    query_embedding = model.encode(
        query,
        convert_to_numpy=True,
        normalize_embeddings=True
    )

    scores = np.dot(corpus_embeddings, query_embedding)
    df["score"] = scores

    region, major = extract_intent(user_query)

    df_filtered = df.copy()

    # ğŸ”¥ ì§€ì—­ í•„í„°
    if region and "ì§€ì—­" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["ì§€ì—­"]
            .astype(str)
            .str.contains(region, na=False)
        ]

    # ğŸ”¥ í•™ê³¼ëª… í•„í„°
    if major and "í•™ê³¼ëª…" in df_filtered.columns:
        df_filtered = df_filtered[
            df_filtered["í•™ê³¼ëª…"]
            .astype(str)
            .str.contains(major, case=False, na=False)
        ]

    return df_filtered.sort_values(
        "score",
        ascending=False
    ).head(top_k)


# ======================================================
# 7. GPT í”„ë¡¬í”„íŠ¸ ìƒì„±
# ======================================================
def build_gpt_prompt(user_query: str, results_df: pd.DataFrame):
    context = ""

    for _, row in results_df.iterrows():
        context += f"""
ëŒ€í•™ëª…: {row.get('ëŒ€í•™ëª…', '')}
í•™ê³¼ëª…: {row.get('í•™ê³¼ëª…', '')}
ì†Œì¬ì§€: {row.get('ì§€ì—­', '')}
í•™ê³¼íŠ¹ì„±: {row.get('í•™ê³¼íŠ¹ì„±', '')}
í‘œì¤€ê³„ì—´: {row.get('í‘œì¤€ë¶„ë¥˜ê³„ì—´(ì¤‘)', '')}
---
"""

    return f"""
ë„ˆëŠ” í•œêµ­ì˜ ëŒ€í•™ ì…ì‹œ ë° ì§„ë¡œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[ì¶”ì²œ ê°€ëŠ¥í•œ í•™ê³¼ ì •ë³´]
{context}

ìš”ì²­ì‚¬í•­:
1. ì§ˆë¬¸ì— ë§ëŠ” í•™ê³¼ë¥¼ ì¶”ì²œí•´ë¼
2. ê° í•™ê³¼ì˜ íŠ¹ì§•ì„ ì‰½ê²Œ ì„¤ëª…í•´ë¼
3. ì¡¸ì—… í›„ ì§„ë¡œì™€ ì „ë§ì„ ì„¤ëª…í•´ë¼
4. ì¹œì ˆí•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ë¼
5. ì œê³µëœ ì •ë³´ ì™¸ì˜ ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆë¼
"""


# ======================================================
# 8. GPT í˜¸ì¶œ
# ======================================================
def call_gpt4_mini(prompt: str) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ëŒ€í•™ ì…ì‹œ ì „ë¬¸ ìƒë‹´ ì±—ë´‡ì´ë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.4
    )
    return response.choices[0].message.content


# ======================================================
# 9. API ìš”ì²­ / ì‘ë‹µ ëª¨ë¸
# ======================================================
class ChatRequest(BaseModel):
    question: str


class ChatResponse(BaseModel):
    answer: str


# ======================================================
# 10. í•µì‹¬ API ì—”ë“œí¬ì¸íŠ¸
# ======================================================
@app.post("/chat", response_model=ChatResponse)
def chat(req: ChatRequest):
    results = search_major(req.question)

    if results.empty:
        return ChatResponse(
            answer="ì¡°ê±´ì— ë§ëŠ” í•™ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë°”ê¿”ë³´ì„¸ìš”."
        )

    prompt = build_gpt_prompt(req.question, results)
    answer = call_gpt4_mini(prompt)

    return ChatResponse(answer=answer)
